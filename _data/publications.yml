
- title: 'Answer Matching Outperforms Multiple Choice for Language Model Evaluations'
  authors:
    - Nikhil Chandak*
    - Shashwat Goel*
    - Ameya Prabhu
    - Moritz Hardt
    - Jonas Geiping
  venue: 'ICML Workshop on Assessing World Models'
  date: 2025-06-11
  type: paper

- title: 'Pitfalls in Evaluating Language Model Forecasters'
  authors:
    - Daniel Paleka*
    - Shashwat Goel*
    - Jonas Geiping
    - Florian Tramèr
  venue: 'ICML Workshop on Assessing World Models'
  date: 2025-06-11
  paper_link: https://arxiv.org/abs/2506.00723
  type: paper

- title: 'Measuring Belief Updates in Curious Agents'
  authors:
    - Joschka Strüber
    - Ilze Amanda Auzina
    - Shashwat Goel
    - Susanne Keller
    - Jonas Geiping
    - Ameya Prabhu
    - Matthias Bethge
  venue: 'ICML Workshop on Assessing World Models'
  date: 2025-06-11
  type: paper

- title: 'Great Models Think Alike and this Undermines AI Oversight'
  authors:
    - Shashwat Goel
    - Joschka Strüber
    - Ilze Amanda Auzina
    - Karuna Chandra
    - P. Kumaraguru
    - Douwe Kiela
    - Ameya Prabhu
    - Matthias Bethge
    - Jonas Geiping
  venue: '(Spotlight) ICML 2025'
  date: 2025-02-06
  paper_link: https://arxiv.org/abs/2502.04313
  links: 
    code: https://github.com/model-similarity/lm-similarity
    tool: https://huggingface.co/spaces/bethgelab/lm-similarity
    data: https://huggingface.co/datasets/bethgelab/lm-similarity
  type: paper

- title: 'Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample Creation'
  authors:
    - Shiven Sinha
    - Shashwat Goel
    - Ponnurangam Kumaraguru
    - Jonas Geiping
    - Matthias Bethge
    - Ameya Prabhu
  venue: '(Oral) ICLR Scaling Self Improving Foundation Models Workshop'
  date: 2025-02-26
  paper_link: https://arxiv.org/abs/2502.19414
  links: 
    webpage: https://falsifiers.github.io
    code: https://github.com/falsifiers/REFUTE
    data: https://huggingface.co/datasets/bethgelab/REFUTE
  type: paper

- title: 'Proportional Aggregation of Preferences for Sequential Decision Making'
  authors:
    - Nikhil Chandak
    - Shashwat Goel
    - Dominik Peters
  venue: 'Outstanding Paper Award (top 3 out of 12,000+ submissions) at 38th Annual Conference of the Association for the Advancement of Artificial Intelligence (AAAI)'
  date: 2024-01-01
  paper_link: https://arxiv.org/abs/2306.14858 
  links: 
    twitter: https://x.com/ShashwatGoel7/status/1760776191139582409 
    talk: https://www.youtube.com/watch?v=kjZG89iDzuU
  type: paper
  
- title: 'Corrective Machine Unlearning'
  authors:
    - Shashwat Goel*
    - Ameya Prabhu*
    - Philip Torr
    - P. Kumaraguru
    - Amartya Sanyal
  venue: 'Transactions on Machine Learning Research (TMLR) 2024 <br> Workshop on Data-centric Machine Learning (DMLR) - Recommended for Journal (Top 15) at the 12th International Conference on Representation Learning (ICLR)'
  date: 2024-01-03
  paper_link: https://arxiv.org/abs/2402.14015
  links: 
    twitter: https://x.com/ShashwatGoel7/status/1760911396034793831
    code: https://github.com/drimpossible/corrective-unlearning-bench
  type: paper
  # description: >
  #   We highlight key distinctions between the application of unlearning to privacy vs removing manipulated or incorrect training data, and we call the latter "Corrective Unlearning". Specifically, the deletion set is not provided by user requests, and model developers may only identify a subset of manipulated samples whose influence is to be deleted. "Retraining on the retain set from scratch'', previously considered a theoretical gold standard, becomes insufficient when the entire manipulated set is not known, re-inforcing the adverse effect of retained manipulated samples. State of the art unlearning procedures aim to approximate this gold standard, leading to existing methods performing poorly when even upto 80\% poisoned samples are identified. We find evidence that corrective unlearning is tractibile, but there is a need for unlearning methods that can handle arbitrary manipulations. 

- title: 'The WMDP Benchmark: Measuring and Reducing Malicious Use with Unlearning'
  authors:
    - Center for AI Safety
    - Scale AI
  venue: 'International Conference on Machine Learning (ICML)'
  date: 2024-01-02
  paper_link: https://arxiv.org/abs/2403.03218
  links: 
    media: https://time.com/6878893/ai-artificial-intelligence-dangerous-knowledge/
    webpage: https://www.wmdp.ai/ 
    code: https://github.com/centerforaisafety/wmdp
  type: paper
  # description: >
  #   We introduce unlearning dual-use knowledge from LLMs as a post-training safety intervention. We release an MCQ benchmark for measuring hazardous knowledge related to the design of bioweapons, cyberattacks, and chemical weapons. We propose RMU, an activation space unlearning method that reduces hazardous knowledge while maintaining overall LLM performance. I designed evaluations and implemented baselines for this project, being involved in ideation from start to finish.


- title: 'Representation Engineering: A Top-Down Approach to AI Transparency'
  authors:
    - Center for AI Safety
  venue: 'ArXiv'
  date: 2023-01-03
  paper_link: https://arxiv.org/abs/2310.01405
  links: 
    talk: https://www.youtube.com/watch?v=2U5NNiGC9yk
    webpage: https://www.ai-transparency.org/
    code: https://github.com/andyzoujm/representation-engineering
  type: paper

- title: 'Probing Negation in Language Models'
  authors:
    - Shashwat Singh*
    - Shashwat Goel*
    - Saujas Vaduguru
    - Ponnurangam Kumaraguru
  venue: '8th Workshop on Representation Learning for NLP (RepL4NLP) <br>61st Annual Meeting of the Association for Computational Linguistics (ACL)'
  date: 2023-01-02
  paper_link: https://shash42.github.io/files/negation.pdf
  links: 
    code: https://github.com/shashwat1002/negation_new 
  type: paper

- title: 'Towards Adversarial Evaluations of Inexact Machine Unlearning'
  authors:
    - Shashwat Goel*
    - Ameya Prabhu*
    - Amartya Sanyal
    - Ser-Nam Lim
    - Phillip Torr
    - Ponnurangam Kumaraguru
  venue: 'ArXiv'
  date: 2023-01-01
  paper_link: https://arxiv.org/abs/2201.06640 
  links: 
    code: https://github.com/shash42/Evaluating-Inexact-Unlearning
  type: paper

# - title: 'Low Impact Agency: Review and Discussion'
#   authors:
#     - Danilo Naiff
#     - Shashwat Goel
#   venue: 'ArXiv'
#   date: 2022-01-01
#   paper_link: https://arxiv.org/abs/2303.03139
#   links: 
#   type: paper

# - title: 'Modelling and Optimizing the Allocation of COVID-19 Swabs to Labs'
#   authors:
#     - Nikhil Chandak
#     - Shashwat Goel
#     - Kunal Jain
#     - Arpan Dasgupta
#   venue: 'Student Abstract at 18th Mixed Integer Programming Workshop<br>Winner, Covid-19 Swabs2Labs Hackathon by Ministry of Health Karnataka'
#   date: 2021-01-02
#   paper_link: https://github.com/shash42/cnihack/blob/master/Report.pdf
#   links: 
#     code: https://github.com/shash42/cnihack
#   type: technical report

# - title: 'Bilingual Dictionary Generation and Enrichment via Graph Exploration'
#   authors:
#     - Shashwat Goel
#     - Jorge Gracia
#     - Mikel L. Forcada
#   venue: 'Special Issue on Latest Advancements in Linguistic Linked Data<br>Semantic Web Journal'
#   date: 2022-01-02
#   paper_link: https://content.iospress.com/articles/semantic-web/sw222899 
#   links: 
#     code: https://github.com/shash42/ApertiumBidixGen
#   type: paper

# - title: 'From Pivots to Graphs: Augmented Cycle Density as a generalization to One Time Inverse Consultation'
#   authors:
#     - Shashwat Goel
#     - Kunwar Shanjeet Grover
#   venue: '4th Shared Task on Translation Inference Across Dictionaries<br>3rd Conference on Language, Data and Knowledge'
#   date: 2021-01-01
#   paper_link: https://arxiv.org/abs/2108.12459 
#   links: 
#   type: paper
